# <h1 align=center> **Bienvenidos a mi PI - Machine Learning** :construction_worker:</h1>

INTRODUCCIÃ“N: :arrow_down:

DescripciÃ³n del problema 
***
Un importante Centro de Salud lo ha contratado con el fin de poder predecir si un paciente tendrÃ¡ una estancia hospitalaria prolongada o no, utilizando la informaciÃ³n contenida en el dataset asociado, la cual recaba una muestra histÃ³rica de sus pacientes, para poder administrar la demanda de camas en el hospital segÃºn la condiciÃ³n de los pacientes recientemente ingresados.

Para esto, se define que un paciente posee estancia hospitalaria prolongada si ha estado hospitalizado mÃ¡s de 8 dÃ­as. Por lo que debe generar dicha variable categÃ³rica y luego categorizar los pacientes segÃºn las variables que usted considere necesarias, justificando dicha elecciÃ³n.â€‹

En este repo encontraran:

ğŸ”¸ Archivo principal comentado: datathon.ipynb

ğŸ”¸ Archivo con los datasets: datasets/....

ğŸ”¸ Archivo con predicciones: Jucepos.csv

***
OBJETIVOS: :arrow_down:

â€‹ Se proveen los siguientes archivos para realizar el proyecto:

'hospitalizaciones_train.csv': Contiene 410000 registros y 15 dimensiones, el cual incluye la informaciÃ³n numÃ©rica de la cantidad de dÃ­as de estancia hospitalaria.
'hospitalizaciones_test.csv': Contiene 90000 registros y 14 dimensiones, el cual no incluye la informaciÃ³n de la cantidad de dÃ­as de estancia hospitalaria.â€‹

***
:bangbang: Resolucion :bangbang:

En este caso decidi hacer una tÃ©cnica de aprendizaje automÃ¡tico supervisada basada en Ã¡rboles de decisiÃ³n (Random Forest).

Uno de los problemas que aparecÃ­a con la creaciÃ³n de un Ã¡rbol de decisiÃ³n es que si le damos la profundidad suficiente, el Ã¡rbol tiende a â€œmemorizarâ€ las soluciones en vez de generalizar el aprendizaje. Es decir, a padecer de overfitting. La soluciÃ³n para evitar esto es la de crear muchos Ã¡rboles y que trabajen en conjunto.

Ventajas
***
ğŸ”¸ Funciona bien -aÃºn- sin ajuste de hiperparÃ¡metros
ğŸ”¸ Funciona bien para problemas de clasificaciÃ³n y tambiÃ©n de regresiÃ³n.
ğŸ”¸ Al utilizar mÃºltiples Ã¡rboles se reduce considerablemente el riesgo de overfiting

Desventajas
***
ğŸ”¸ Es mucho mÃ¡s â€œcostoâ€ de crear y ejecutar que â€œun sÃ³lo Ã¡rbolâ€ de decisiÃ³n.
ğŸ”¸ Puede requerir muchÃ­simo tiempo de entrenamiento
ğŸ”¸ Random Forest no funciona bien con datasets pequeÃ±os.

***
Herramientas utilizadas: :arrow_down:

ğŸ”¸ Python

ğŸ”¸ Pandas

ğŸ”¸ Matplotlib

ğŸ”¸ Seaborn

ğŸ”¸ Scikit-learn

